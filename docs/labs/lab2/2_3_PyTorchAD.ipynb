{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "97d3c80864403311b3342877098cef5b",
          "grade": false,
          "grade_id": "cell-f38ec230aeb63f9d",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "XoB-DpopQMMQ"
      },
      "source": [
        "# Part 3: Reverse Mode Automatic Differentiation with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "529f94d77025a0b9b2a4aa44ed0b1f1b",
          "grade": false,
          "grade_id": "cell-2afa7eb0f5479ff9",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "W4ZcaYwrQMMU"
      },
      "outputs": [],
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8c13a801a6e49369b91bda4e906cb1cf",
          "grade": false,
          "grade_id": "cell-3d5d3b3ff9e2ac9f",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "rcw6IV8kQMMW"
      },
      "source": [
        "PyTorch implements Dynamic Reverse Mode Automatic Differentiation, much like we did in the previous exercise. There is one really major difference in what PyTorch provides over our simple example: it works directly with matrices (`Tensor`s) rather than with scalars (although obviously a matrix can represent a scalar).\n",
        "\n",
        "In this tutorial, we'll explore PyTorch's AD implementation. Note that we're using the API of PyTorch 0.4 or later which simplifies use of AD (previous versions required wrapping all `Tensor`s that you wanted to compute gradients of in `Variable` objects; PyTorch 0.4 removes the need to do this and allows `Tensor`s themselves to track gradients).\n",
        "\n",
        "We'll start with the simple example we tried earlier in the code block below:\n",
        "\n",
        "__Task:__ Run the following code and verify the solution is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0d5aa67ad79542449beb79e8b65a50bc",
          "grade": false,
          "grade_id": "cell-a7740c83bfcaa983",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGKwMNWpQMMX",
        "outputId": "0aa06a6a-1b82-450a-ce89-5f5624b5b12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z = 2.57942533493042\n",
            "dz/dx = 5.077582359313965\n",
            "dz/dy = 0.5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# set up the problem\n",
        "x = torch.tensor(0.5, requires_grad=True) #set requires_grad = True to track gradient\n",
        "y = torch.tensor(4.2, requires_grad=True)\n",
        "z = x * y + torch.sin(x)\n",
        "\n",
        "print(\"z = \" + str(z.item()))\n",
        "\n",
        "z.backward() # this goes through the computation graph and accumulates the gradients in the cached .grad attributes\n",
        "print(\"dz/dx = \" + str(x.grad.item()))\n",
        "print(\"dz/dy = \" + str(y.grad.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "aac08494fe446009596079eba3016ddf",
          "grade": false,
          "grade_id": "cell-cb3a2586467dd0ad",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "2P-UZBnaQMMY"
      },
      "source": [
        "As with our own AD implementation, PyTorch lets us differentiate through an algorithm.\n",
        "\n",
        "__Task__: Use the block below to compute the gradient $\\partial z/\\partial x$ of the following pseudocode algorithm and store the result in the `dzdx` variable:\n",
        "\n",
        "    x = 0.5\n",
        "    z = 1\n",
        "    i = 0\n",
        "    while i<2:\n",
        "        z = (z + i) * x * x\n",
        "        i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b689f4aaf1507a7f532c6059896ad02e",
          "grade": false,
          "grade_id": "cell-62e1f0838236eddf",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "3ODAAzgjQMMY"
      },
      "outputs": [],
      "source": [
        "# set up the problem\n",
        "x = torch.tensor(0.5, requires_grad=True) #set requires_grad = True to track gradient\n",
        "z = torch.tensor(1.0, requires_grad=True)\n",
        "i = 0\n",
        "while(i < 2):\n",
        "  z = (z + torch.tensor(float(i), requires_grad=True)) * x * x\n",
        "\n",
        "print(\"z = \" + str(z.item()))\n",
        "\n",
        "z.backward() # this goes through the computation graph and accumulates the gradients in the cached .grad attributes\n",
        "print(\"dz/dx = \" + str(x.grad.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bc79555ad292cd1026a9db951f9f8e47",
          "grade": true,
          "grade_id": "cell-cbe71552690f710b",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false
        },
        "id": "iY0P0UXsQMMZ"
      },
      "outputs": [],
      "source": [
        "assert dzdx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d999d1c4c84b066fb15be1972dcaaad8",
          "grade": false,
          "grade_id": "cell-824ecf46b4875b5f",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "mAL0j5-EQMMZ"
      },
      "source": [
        "## PyTorch limitations: in-place operations and aliasing\n",
        "\n",
        "PyTorch will throw an error at runtime if you try to differentiate through an in-place operation on a tensor. \n",
        "\n",
        "__Task__: Run the following code to see this in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "eb07b791962399cd736ede26b99309e8",
          "grade": false,
          "grade_id": "cell-813341e9825354bf",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "uaa96-3BQMMa",
        "outputId": "d4542184-cb18-474a-a0e4-003fa777bf1a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-637e22e4c834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# inplace addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of TanhBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ],
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "y.add_(3) # inplace addition\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在计算y的时候, x是等于某个值的, y对于x的导数是和这时候的x值相关的\n",
        "但是计算完add_(3)之后, y的值被in-place, 这就会导致 f.backward() 对于 x的导数计算出错误, 为了防止这种错误, pytorch 选择了报错的形式.\n",
        "造成这个问题的主要原因是因为 在执行 y = x.tanh() 这句的时候, pytorch 的反向求导机制 保存了x的引用为了之后的 反向求导计算."
      ],
      "metadata": {
        "id": "-4tTr2L7aKZB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bfd84ec33f62c537a0ea6ea4f87291de",
          "grade": false,
          "grade_id": "cell-b9986c88256e8468",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "WUtsjdxzQMMb"
      },
      "source": [
        "Aliasing is also something that can't be differentiated through and will result in a slightly more cryptic error.\n",
        "\n",
        "__Task__: Run the following code to see this in action. If you don't understand what this code does add some `print` statements to show the values of `x` and `y` at various points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "3TVh81OFQMMc",
        "outputId": "a7bb9b63-97a4-4dba-b5c1-d8ac67322cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3., 4.], requires_grad=True)\n",
            "tensor([1.], grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8a5bb62b5e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
          ]
        }
      ],
      "source": [
        "x = torch.tensor([1, 2, 3, 4], requires_grad=True, dtype=torch.float)\n",
        "print(x)\n",
        "y = x[:1]\n",
        "print(y)\n",
        "y.add_(3)\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4e749620afa15537eddb1f61565c927e",
          "grade": false,
          "grade_id": "cell-76e1b1d7cbc02388",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "dakQqAdQQMMc"
      },
      "source": [
        "## Dealing with multiple outputs\n",
        "\n",
        "PyTorch can deal with the case where there are multiple output variables if we can formulate the expression in terms of tensor operations. Consider the example from the presentation for example:\n",
        "\n",
        "$$\\begin{cases}\n",
        "     z = 2x + \\sin x\\\\\n",
        "     v = 4x + \\cos x\n",
        "\\end{cases}$$\n",
        "\n",
        "We could formulate this as:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}z \\\\ v\\end{bmatrix} = \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} \\odot \\bar{x} + \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} \\odot \\sin\\bar x + \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} \\odot \\cos\\bar x\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$$\n",
        "\\bar x = \\begin{bmatrix}x \\\\ x\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "and $\\odot$ represents the Hadamard or element-wise product. This is demonstrated using PyTorch in the following code block.\n",
        "\n",
        "__Task:__ run the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5ab1d7ea5f6b1a017f7949fa0dd1c8f1",
          "grade": false,
          "grade_id": "cell-58a1e0676df73645",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upIog4y6QMMd",
        "outputId": "b3368dcf-8e56-4f1b-f401-20d19981d7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.5403],\n",
            "        [3.1585]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1.0],[1.0]], requires_grad=True)\n",
        "\n",
        "zv = ( torch.tensor([[2.0],[4.0]]) * x +\n",
        "         torch.tensor([[1.0], [0.0]]) * torch.sin(x) +\n",
        "         torch.tensor([[0.0], [1.0]]) * torch.cos(x) )\n",
        "        \n",
        "zv.backward(torch.tensor([[1.0],[1.0]])) # Note as we have \"multiple outputs\" we must pass in a tensor of weights of the correct size\n",
        "\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1e10ee0f8f8395414cb70aa241376003",
          "grade": true,
          "grade_id": "cell-b85be7c90ef1f408",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true
        },
        "id": "9d4YDG6KQMMe"
      },
      "source": [
        "The gradient is related to initial values of inputs since gradients sometimes are functions of some inputs and neurons of hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD6EPbhtQMMe"
      },
      "source": [
        "## Gradient descent & gradients with respect to a vector\n",
        "Let's put everything together and using automatically computed gradients to find the minima of a function by taking steps down the gradient from an initial position. Rather than explicitly creating each input variable as a scalar as in the previous examples, we'll use a vector instead (so our gradients will be with respect to each element of the vector).\n",
        "\n",
        "__Task:__ work through the following example to see how taking gradients with respect to a vector works & how simple gradient descent optimisation can be implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_mnLWyfQMMf",
        "outputId": "c13b1807-0bbb-4a3f-8be3-7301aeb764db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "(None, 67.19999694824219)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 0.7747630476951599)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 0.008932411670684814)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 0.00010298370034433901)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 1.1873213452417986e-06)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 1.3688886468798955e-08)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 1.5782215811999123e-10)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 1.8195657654207498e-12)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 2.0978168543115717e-14)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "(None, 2.418617877589929e-16)\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# This is our starting vector\n",
        "initial=[[2.0], [1.0], [10.0]]\n",
        "\n",
        "# This is the function we will optimise (feel free to work out the analytic minima!)\n",
        "def function(x):\n",
        "    return x[0]**2 + x[1]**2 + x[2]**2\n",
        "\n",
        "x = torch.tensor(initial, requires_grad=True, dtype=torch.float)\n",
        "for i in range(0,100):\n",
        "    # manually dispose of the gradient (in reality it would be better to detach and zero it to reuse memory)\n",
        "    x.grad = None\n",
        "    # evaluate the function\n",
        "    J = function(x) \n",
        "    # auto-compute the gradients at the previously evaluated point x\n",
        "    J.backward()\n",
        "    # compute the update\n",
        "    x = x - x.grad*0.1 \n",
        "    print(x.requires_grad)\n",
        "    if i%10 == 0:\n",
        "        print((x.grad_fn, function(x).item()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to show the use of requires_grad\n",
        "#also, we can use requires_grad_ to in-place\n",
        "a = torch.randn((2, 2), requires_grad=True)\n",
        "print(a)\n",
        "print(a.requires_grad)\n",
        "a = ((a * 3) / (a - 1))\n",
        "print(a.requires_grad)\n",
        "a.requires_grad_(False)\n",
        "print(a.requires_grad)\n",
        "b = (a * a).sum()\n",
        "print(b.requires_grad)\n",
        "print(b.grad_fn)"
      ],
      "metadata": {
        "id": "iv1Qc35HPDY_",
        "outputId": "0b8e53d0-2430-4509-ecc3-35162c21d48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3982,  1.1602],\n",
            "        [-0.8490,  1.8391]], requires_grad=True)\n",
            "True\n",
            "True\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-efba4de8dab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach()."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1a6fb5c11feed5015f3836b04ed25d65",
          "grade": false,
          "grade_id": "cell-fe38c3ff8580d8e1",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "a_L-xDH6QMMf"
      },
      "source": [
        "__Task__: Answer the following question in the box below: Why must the update in the code above be assigned to `x.data` rather than `x`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2b3a76629b34fb1c8a80c90d9df28dd9",
          "grade": true,
          "grade_id": "cell-855b4580505f3b5c",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true
        },
        "id": "j0jDxf3oQMMf"
      },
      "source": [
        "tensor.data的两点总结：\n",
        "\n",
        "（1）tensor .data 返回和 x 的相同数据 tensor,而且这个新的tensor和原来的tensor是共用数据的，一者改变，另一者也会跟着改变，而且新分离得到的tensor的require s_grad = False, 即不可求导的。（这一点其实detach是一样的）\n",
        "\n",
        "（2）使用tensor.data的局限性。文档中说使用tensor.data是不安全的, 因为 x.data 不能被 autograd 追踪求微分 。什么意思呢？由于我更改分离之后的变量值,导致原来的张量值也跟着改变了，但是这种改变对于autograd是没有察觉的，它依然按照求导规则来求导，导致得出完全错误的导数值却浑然不知。它的风险性就是如果我再任意一个地方更改了某一个张量，求导的时候也没有通知我已经在某处更改了，导致得出的导数值完全不正确，故而风险大。\n",
        "\n",
        "tensor.detach()的两点总结：\n",
        "\n",
        "（1）tensor .detach() 返回和 x 的相同数据 tensor,而且这个新的tensor和原来的tensor是共用数据的，一者改变，另一者也会跟着改变，而且新分离得到的tensor的require s_grad = False, 即不可求导的。（这一点其实 .data是一样的）\n",
        "\n",
        "（2）使用tensor.detach()的优点。从上面的例子可以看出，由于我更改分离之后的变量值,导致原来的张量out的值也跟着改变了，这个时候如果依然按照求导规则来求导，由于out已经更改了，所以不会再继续求导了，而是报错，这样就避免了得出完全牛头不对马嘴的求导结果。\n",
        "\n",
        "相同点：tensor.data和tensor.detach() 都是变量从图中分离，但而这都是“原位操作 inplace operation”。\n",
        "\n",
        "不同点：\n",
        "\n",
        "（1）.data 是一个属性，二.detach()是一个方法；\n",
        "\n",
        "（2）.data 是不安全的，.detach()是安全的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBv_Gs_hQMMg"
      },
      "source": [
        "## Differentiating through random operations\n",
        "\n",
        "We'll end with an example that will be important later in the course: differentiating with respect to the parameters of a random number generator.\n",
        "\n",
        "Assume that as some part of a differentiable program that we write we wish to incorporate a random element where we sample values, $z$ from a Normal distribution: $z \\sim \\mathcal{N}(\\mu,\\sigma^2)$. We want to learn the parameters of the generator $\\mu$ and $\\sigma^2$, but how can we do this? In a differentiable program setting we want to differentiate with respect to these parameters, but at first glance it isn't at all obvious what this means as the generator _just_ produces numbers which themselves don't have gradients.\n",
        "\n",
        "The answer is often called the _reparameterisation trick_: ***If we note that sampling a Normal distribution with a specfic mean and variance is equivalent to drawing numbers from a standard Normal distribution and scaling and shifting them: $z \\sim \\mathcal{N}(\\mu,\\sigma^2) \\equiv z \\sim \\mu + \\sigma\\mathcal{N}(0,1)\\equiv z = \\mu + \\sigma \\zeta\\, \\rm{where}\\, \\zeta\\sim\\mathcal{N}(0,1)$. With this reparameterisation the gradients with respect to the parameters are obvious.***\n",
        "\n",
        "The following code block demonstrates this in practice; each of the gradients can be interpreted as how much an infintesimal change in $\\mu$ or $\\sigma$ would change $z$ if we could repeat the sampling operation again with the same value of `torch.randn(1)` being produced:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsk-HFX8QMMg",
        "outputId": "098cf502-e92b-4357-df22-0b68e7576e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z: 1.2051912546157837 \tdzdmu: 1.0 \tdzdsigma: 0.2051912248134613\n",
            "z: 1.2875546216964722 \tdzdmu: 1.0 \tdzdsigma: 0.28755462169647217\n",
            "z: -2.492189645767212 \tdzdmu: 1.0 \tdzdsigma: -3.492189645767212\n",
            "z: -1.1215157508850098 \tdzdmu: 1.0 \tdzdsigma: -2.1215157508850098\n",
            "z: 0.45651155710220337 \tdzdmu: 1.0 \tdzdsigma: -0.5434884428977966\n",
            "z: 1.753296971321106 \tdzdmu: 1.0 \tdzdsigma: 0.753296971321106\n",
            "z: 0.3965885639190674 \tdzdmu: 1.0 \tdzdsigma: -0.6034114360809326\n",
            "z: -0.6901881694793701 \tdzdmu: 1.0 \tdzdsigma: -1.6901881694793701\n",
            "z: 1.898858666419983 \tdzdmu: 1.0 \tdzdsigma: 0.8988586664199829\n",
            "z: -0.9623163938522339 \tdzdmu: 1.0 \tdzdsigma: -1.9623163938522339\n"
          ]
        }
      ],
      "source": [
        "mu = torch.tensor(1.0, requires_grad=True)\n",
        "sigma = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "for i in range(10):\n",
        "    mu.grad = None\n",
        "    sigma.grad = None\n",
        "    \n",
        "    z = mu + sigma * torch.randn(1) #1 row 1 column\n",
        "    z.backward()\n",
        "    print(\"z:\", z.item(), \"\\tdzdmu:\", mu.grad.item(), \"\\tdzdsigma:\", sigma.grad.item())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "2_3_PyTorchAD.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}